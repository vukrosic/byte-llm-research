# Byte-Level Language Model

A minimal transformer-based language model that operates at the byte level, predicting the next byte in a sequence instead of tokens.

## Features

- **Byte-level processing**: No tokenizer needed, works with raw UTF-8 bytes
- **Interactive inference**: Real-time text generation with configurable parameters
- **Full context utilization**: Uses maximum context window for coherent generation
- **Flexible sampling**: Temperature, top-k, top-p, and repetition penalty controls

## Quick Start

### 1. Install Dependencies

```bash
pip install torch datasets transformers tqdm numpy
```

### 2. Train the Model

```bash
python train.py
```

This will:
- Download and process text data
- Train a byte-level transformer model
- Save the trained model as `byte_llm_model.pt`

### 3. Run Interactive Inference

```bash
python inference.py byte_llm_model.pt
```

### 4. Try the Demo

```bash
python demo_inference.py
```

## Model Architecture

- **Vocabulary**: 256 bytes (0-255)
- **Context Length**: 512 bytes
- **Architecture**: 6-layer transformer with 8 attention heads
- **Parameters**: ~2.5M parameters
- **Optimizer**: Muon (momentum orthogonalized by Newton-Schulz)

## Interactive Commands

In interactive mode, you can use these commands:

- `/help` - Show available commands
- `/config` - Display current generation settings
- `/temp X` - Set temperature (0.1-2.0, default: 0.8)
- `/topk X` - Set top-k sampling (1-256, default: 50)
- `/topp X` - Set top-p/nucleus sampling (0.1-1.0, default: 0.9)
- `/length X` - Set max generation length in bytes (default: 1024)
- `/quit` - Exit the program

## Generation Parameters

- **Temperature**: Controls randomness (lower = more focused, higher = more creative)
- **Top-k**: Only consider top k most likely next bytes
- **Top-p**: Only consider bytes with cumulative probability up to p
- **Repetition Penalty**: Reduces likelihood of repeating recent bytes

## Example Usage

```python
from inference import ByteLevelGenerator, InferenceConfig

# Create config
config = InferenceConfig(
    max_new_bytes=512,
    temperature=0.8,
    top_k=50,
    top_p=0.9
)

# Load model and generate
generator = ByteLevelGenerator("byte_llm_model.pt", config)
result = generator.generate("The future of AI is")
print(result)
```

## How It Works

1. **Input Processing**: Text is converted to UTF-8 bytes (0-255)
2. **Model Forward**: Transformer predicts probability distribution over next byte
3. **Sampling**: Next byte is sampled using temperature/top-k/top-p
4. **Context Management**: Sliding window maintains full context utilization
5. **Output Decoding**: Generated bytes are converted back to text

## Advantages of Byte-Level

- **Universal**: Works with any language/script without tokenization
- **No OOV**: Never encounters out-of-vocabulary issues
- **Fine-grained**: Can learn character-level and sub-character patterns
- **Simple**: No complex tokenizer preprocessing needed

## Files

- `train.py` - Training script with Muon optimizer
- `inference.py` - Interactive inference script
- `demo_inference.py` - Demo generation examples
- `README.md` - This documentation

## Training Details

- **Dataset**: HuggingFace SmolLM corpus (cosmopedia-v2)
- **Sequence Length**: 512 bytes
- **Batch Size**: 24
- **Training Steps**: 5,000
- **Learning Rate**: 0.01 (Muon), 0.001 (AdamW for embeddings/norms)
- **Mixed Precision**: Enabled for faster training

## Performance Tips

- Use CUDA GPU for faster training and inference
- Adjust `max_new_bytes` based on your needs (longer = more coherent but slower)
- Lower temperature for more focused output, higher for more creative
- Use top-k=1 for deterministic (greedy) generation

## Troubleshooting

**Model file not found**: Make sure to train the model first with `python train.py`

**CUDA out of memory**: Reduce batch size in `ModelConfig` or use CPU

**Garbled output**: The model may need more training, or try adjusting generation parameters

**Slow generation**: Use GPU if available, or reduce `max_new_bytes`